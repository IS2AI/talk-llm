# sample config to run a demo training of 10 epochs

data_root: ./data_issai/
train_list_file: ./data_issai/training_list.txt
val_list_file: ./data_issai/validation_list.txt
test_list_file: ./data_issai/testing_list.txt
label_map: ./data_issai/label_map.json

exp:
    wandb: False
    wandb_api_key:
    proj_name: torch-kw-mlp
    exp_dir: ./runs
    exp_name: kw-mlp-issai-0.1
    device: auto
    log_freq: 20  # steps
    log_to_file: False
    log_to_stdout: True
    val_freq: 1   # epochs
    n_workers: 32
    pin_memory: True
    cache: 2      # 0 -> no cache | 1 -> cache wavs | 2 -> cache specs; stops wav augments
    

hparams:
    restore_ckpt: #runs/kw-mlp-0.1.0/best.pth
    seed: 0
    batch_size: 256
    start_epoch: 0
    n_epochs: 100
    l_smooth: 0.1

    audio:
        sr: 16000
        n_mels: 40
        n_fft: 480
        win_length: 480
        hop_length: 160
        center: False
    
    model:
        type: kw-mlp 
        input_res: [40, 98]
        patch_res: [40, 1]
        num_classes: 2
        channels: 1
        dim: 64
        depth: 6 #12
        pre_norm: False
        prob_survival: 0.9

    optimizer:
        opt_type: adamw
        opt_kwargs:
          lr: 0.001
          weight_decay: 0.1
    
    scheduler:
        n_warmup: 10
        max_epochs: 140
        scheduler_type: cosine_annealing

    augment:
        spec_aug:
            n_time_masks: 2
            time_mask_width: 15
            n_freq_masks: 2
            freq_mask_width: 5
